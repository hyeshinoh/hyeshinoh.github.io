<!DOCTYPE html>
<html lang=en>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="google-site-verification" content="gKrpPp3pdUTTicqIbNv6e3j6c232Ae4ouHGND_G68cI">
    <meta name="description" content="1. 선형 회귀 모형(linear regression)1.1 선형 회귀 모형이란 선형 회귀 모형이란 독립 변수 $x$에서 종속 변수 $y$를 예측하기 위한 방법의 하나로 독립 변수 벡터 $x$와 가중치 벡터 $w$의 가중합으로 $y$와 가장 비슷한 값 $\hat y$을 계산  $$ \hat y = w_1x_1 + \cdots + w_Nx_N $$ $$ \h">
<meta name="keywords" content="data analysis,math,선형대수">
<meta property="og:type" content="article">
<meta property="og:title" content="&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)">
<meta property="og:url" content="https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/index.html">
<meta property="og:site_name" content="우와팬더의 블로그">
<meta property="og:description" content="1. 선형 회귀 모형(linear regression)1.1 선형 회귀 모형이란 선형 회귀 모형이란 독립 변수 $x$에서 종속 변수 $y$를 예측하기 위한 방법의 하나로 독립 변수 벡터 $x$와 가중치 벡터 $w$의 가중합으로 $y$와 가장 비슷한 값 $\hat y$을 계산  $$ \hat y = w_1x_1 + \cdots + w_Nx_N $$ $$ \h">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2020-02-02T12:00:07.459Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)">
<meta name="twitter:description" content="1. 선형 회귀 모형(linear regression)1.1 선형 회귀 모형이란 선형 회귀 모형이란 독립 변수 $x$에서 종속 변수 $y$를 예측하기 위한 방법의 하나로 독립 변수 벡터 $x$와 가중치 벡터 $w$의 가중합으로 $y$와 가장 비슷한 값 $\hat y$을 계산  $$ \hat y = w_1x_1 + \cdots + w_Nx_N $$ $$ \h">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon_panda.png">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon_panda.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon_panda.png">
          
        
    
    <!-- title -->
    <title>&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
      <link rel="alternate" href="/atom.xml" title="우와팬더의 블로그" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>

<body class="max-width mx-auto px3 ltr">    
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/categories/Diary">Diary</a></li>
         
          <li><a href="/categories/DataScience/">Data Science</a></li>
         
          <li><a href="/categories/Life/">Life</a></li>
         
          <li><a href="/categories/">Categories</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2018/11/21/math-lin_alg-018/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2018/11/16/math-lin_alg-016/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&text=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&title=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&is_video=false&description=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)&body=Check out this article: https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&title=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&title=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&title=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&title=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&name=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-선형-회귀-모형-linear-regression"><span class="toc-number">1.</span> <span class="toc-text">1. 선형 회귀 모형(linear regression)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-선형-회귀-모형이란"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 선형 회귀 모형이란</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-잔차"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 잔차</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-잔차-제곱합-RSS-residual-sum-of-squares"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 잔차 제곱합(RSS: residual sum of squares)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-연립방정식과-역행렬"><span class="toc-number">2.</span> <span class="toc-text">2. 연립방정식과 역행렬</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-선형-연립-방정식-system-of-linear-equations"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 선형 연립 방정식(system of linear equations)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-역행렬"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 역행렬</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-역행렬과-선형-연립-방정식의-해"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 역행렬과 선형 연립 방정식의 해</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-최소-자승-문제-Least-Square-Problem"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 최소 자승 문제 (Least Square Problem)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#미지수의-수와-방정식의-수를-고려한-연립-방정식의-종류"><span class="toc-number">2.4.1.</span> <span class="toc-text">미지수의 수와 방정식의 수를 고려한 연립 방정식의 종류</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#최소-자승-문제"><span class="toc-number">2.4.2.</span> <span class="toc-text">최소 자승 문제</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#의사-역행렬-pseudo-inverse"><span class="toc-number">2.4.3.</span> <span class="toc-text">의사 역행렬(pseudo inverse)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#참고-자료"><span class="toc-number">2.4.4.</span> <span class="toc-text">참고 자료</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        &lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">우와팬더의 블로그</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2018-11-21T02:26:04.000Z" itemprop="datePublished">2018-11-21</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/DataScience/">DataScience</a> › <a class="category-link" href="/categories/DataScience/Math/">Math</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/data-analysis/">data analysis</a>, <a class="tag-link" href="/tags/math/">math</a>, <a class="tag-link" href="/tags/선형대수/">선형대수</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h2 id="1-선형-회귀-모형-linear-regression"><a href="#1-선형-회귀-모형-linear-regression" class="headerlink" title="1. 선형 회귀 모형(linear regression)"></a><strong>1. 선형 회귀 모형(linear regression)</strong></h2><h3 id="1-1-선형-회귀-모형이란"><a href="#1-1-선형-회귀-모형이란" class="headerlink" title="1.1 선형 회귀 모형이란"></a>1.1 선형 회귀 모형이란</h3><ul>
<li>선형 회귀 모형이란 독립 변수 $x$에서 종속 변수 $y$를 예측하기 위한 방법의 하나로 독립 변수 벡터 $x$와 가중치 벡터 $w$의 가중합으로 $y$와 가장 비슷한 값 $\hat y$을 계산</li>
</ul>
<p>$$ \hat y = w_1x_1 + \cdots + w_Nx_N $$</p>
<p>$$ \hat y = w^Tx $$</p>
<h3 id="1-2-잔차"><a href="#1-2-잔차" class="headerlink" title="1.2 잔차"></a>1.2 잔차</h3><ul>
<li>선형 회귀 분석의 결과는 가중치 벡터 $w$로 나타나고 예측치는 이 가중치 벡터를 사용한 벡터 $x_i$의 가중합 $w^Tx_i$가됨</li>
<li>잔차(residual) 혹은 오차(error): 예측치 $\hat {y_i}$와 실제값(target) $y_i$의 차이</li>
</ul>
<p>$$ e_i = y_i - \hat {y_i} = y_i - w^Tx_i $$</p>
<h3 id="1-3-잔차-제곱합-RSS-residual-sum-of-squares"><a href="#1-3-잔차-제곱합-RSS-residual-sum-of-squares" class="headerlink" title="1.3 잔차 제곱합(RSS: residual sum of squares)"></a>1.3 잔차 제곱합(RSS: residual sum of squares)</h3><ul>
<li>잔차의 크기는 잔차 벡터의 잔차제곱합을 이용하여 구하며, $e^Te$로 나타냄</li>
</ul>
<p>$$ \sum_{i=1}^N e_i^2 = \sum_{i=1}^N {(y_i - w^Tx_i)^2} = e^Te = (y-Xw)^T(y-Xw) $$</p>
<h2 id="2-연립방정식과-역행렬"><a href="#2-연립방정식과-역행렬" class="headerlink" title="2. 연립방정식과 역행렬"></a><strong>2. 연립방정식과 역행렬</strong></h2><p>그렇다면 가중치 벡터 $w$는 어떻게 구하는가? 연립방정식과 (의사)역행렬을 이용해 선형 예측 모형의 가중치 벡터를 구하는 방법을 알아보자.</p>
<h3 id="2-1-선형-연립-방정식-system-of-linear-equations"><a href="#2-1-선형-연립-방정식-system-of-linear-equations" class="headerlink" title="2.1 선형 연립 방정식(system of linear equations)"></a>2.1 선형 연립 방정식(system of linear equations)</h3><ul>
<li>선형연립 방정식: $x_1, x_2, \cdots, x_M$이라는 $M$개의 미지수를 가지는 $N$개의 선형 방정식</li>
</ul>
<p>$$ \begin{matrix}<br>a_{11} x_1 &amp; + \;&amp; a_{12} x_2   &amp;\; + \cdots + \;&amp; a_{1M} x_M &amp;\; = \;&amp; b_1 \\<br>a_{21} x_1 &amp; + \;&amp; a_{22} x_2   &amp;\; + \cdots + \;&amp; a_{2M} x_M &amp;\; = \;&amp; b_2 \\<br>\vdots\;\;\; &amp;   &amp; \vdots\;\;\; &amp;                &amp; \vdots\;\;\; &amp;     &amp; \;\vdots \\<br>a_{N1} x_1 &amp; + \;&amp; a_{N2} x_2   &amp;\; + \cdots + \;&amp; a_{NM} x_M &amp;\; = \;&amp; b_N \\<br>\end{matrix}<br>$$</p>
<ul>
<li>행렬을 사용하면 아래와 같이 간단하게 표현할 수 있음<ul>
<li>$A$: 계수행렬(coefficient matrix)</li>
<li>$x$: 미지수벡터(unknown vector)</li>
<li>$b$: 상수벡터(constant vector)</li>
</ul>
</li>
</ul>
<p>$$<br>\begin{bmatrix}<br>a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1M} \\<br>a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2M} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>a_{N1} &amp; a_{N2} &amp; \cdots &amp; a_{NM} \\<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1 \\ x_2 \\ \vdots \\ x_M<br>\end{bmatrix}<br> =<br>\begin{bmatrix}<br>b_1 \\ b_2 \\ \vdots \\ b_N<br>\end{bmatrix}<br>$$</p>
<p>$$Ax  = b  $$</p>
<h3 id="2-2-역행렬"><a href="#2-2-역행렬" class="headerlink" title="2.2 역행렬"></a>2.2 역행렬</h3><ul>
<li>역행렬($A^{-1}$): 정방행렬 A에 대해 다음 관계를 만족하는 정방 행렬 ($I$는 단위 행렬)</li>
</ul>
<p>$$ A^{-1}A = AA^{-1} = I $$</p>
<ul>
<li>역행렬의 성질 ($A, B, C$모두 역행렬이 존재한다고 가정)<ul>
<li>전치 행렬의 역행렬은 역행렬의 전치 행렬과 같다. 따라서 대칭 행렬의 역행렬도 대칭 행렬이다.<br>$$ (A^T)^{-1} = (A^{-1})^T $$</li>
<li>두 개 이상의 정방 행렬의 곱은 같은 크기의 정방 행렬이 되는데, 이러한 행렬의 곱의 역행렬은 다음 성질이 성립한다.<br>$$ (AB)^{-1} = B^{-1}A^{-1} $$<br>$$ (ABC)^{-1} = C^{-1}B^{-1}A^{-1} $$</li>
<li>역행렬은 행렬식이 0이 아닌 경우에만 존재한다.</li>
</ul>
</li>
</ul>
<h3 id="2-3-역행렬과-선형-연립-방정식의-해"><a href="#2-3-역행렬과-선형-연립-방정식의-해" class="headerlink" title="2.3 역행렬과 선형 연립 방정식의 해"></a>2.3 역행렬과 선형 연립 방정식의 해</h3><ul>
<li>행렬 $A$의 역행렬이 존재한다면 선형 연립 방정식의 해는 다음과 같이 구함</li>
</ul>
<p>$$ \begin{align}<br>Ax &amp; = b \\<br>A^{-1}Ax &amp; = A^{-1}b \\<br>Ix &amp; = A^{-1}b \\<br>x &amp; = A^{-1}b \\<br>\end{align} $$</p>
<ul>
<li>역행렬이 존재할 때만 (행렬식이 0이 아닌 경우에만) 구할 수 있음</li>
</ul>
<h3 id="2-4-최소-자승-문제-Least-Square-Problem"><a href="#2-4-최소-자승-문제-Least-Square-Problem" class="headerlink" title="2.4 최소 자승 문제 (Least Square Problem)"></a>2.4 최소 자승 문제 (Least Square Problem)</h3><h4 id="미지수의-수와-방정식의-수를-고려한-연립-방정식의-종류"><a href="#미지수의-수와-방정식의-수를-고려한-연립-방정식의-종류" class="headerlink" title="미지수의 수와 방정식의 수를 고려한 연립 방정식의 종류"></a>미지수의 수와 방정식의 수를 고려한 연립 방정식의 종류</h4><ol>
<li>방정식의 수와 미지수의 수가 같음 ($N = M$)<ul>
<li>정방행렬의 경우</li>
</ul>
</li>
<li>방정식의 수 &lt; 미지수의 수 ($N &lt; M$)<ul>
<li>무수히 많은 해가 존재할 수 있음</li>
</ul>
</li>
<li>방정식의 수 &gt; 미지수의 수 ($N &gt; M$)<ul>
<li>해가 존재하지 않을 수 있음</li>
</ul>
</li>
</ol>
<h4 id="최소-자승-문제"><a href="#최소-자승-문제" class="headerlink" title="최소 자승 문제"></a>최소 자승 문제</h4><ul>
<li>데이터 분석에서는 대부분 큰 데이터를 다루게 되므로 3번의 경우가 일반적이기때문에 정확한 해를 구할 수 없음</li>
<li>아무런 답도 구할 수 없는 것 보다는 좌변과 우변을 가장 비슷하게라도 만들어주는 수를 구하는 것이 좋음</li>
</ul>
<p>$$ Ax \approx b $$</p>
<ul>
<li>따라서 잔차의 크기를 최소화하는 문제로 바꾸어 풀어야 함 <br> 이는 잔차 벡터의 크기, 즉 놈을 최소화하는 것과 같음</li>
</ul>
<p>$$ e = Ax -b $$</p>
<p>$$ e^Te = \Vert e \Vert^2 = (Ax -b)^T(Ax-b) $$</p>
<p>$$ x = arg \min_x e^Te = arg \min_x \Vert e \Vert^2 = (Ax -b)^T(Ax-b) $$</p>
<h4 id="의사-역행렬-pseudo-inverse"><a href="#의사-역행렬-pseudo-inverse" class="headerlink" title="의사 역행렬(pseudo inverse)"></a>의사 역행렬(pseudo inverse)</h4><ul>
<li>전치행렬을 원래 행렬에 곱해주면 정방행렬 형태가 됨 ($A^TA$ or $AA^T$)</li>
<li>이를 이용해 최소 자승 문제를 다음과 같이 의사 역행렬 $A^+$로 풀 수 있음 </li>
</ul>
<p>$$ \begin{align}<br>Ax &amp; = b \\<br>A^TAx &amp; = A^Tb \\<br>(A^TA)^{-1}A^TAx &amp; = (A^TA)^{-1}A^Tb \\<br>x &amp; = (A^TA)^{-1}A^Tb \\<br>x &amp; = A^+b \\<br>\end{align} $$</p>
<ul>
<li>의사 역행렬: $A^+ = (A^TA)^{-1}A^T$</li>
</ul>
<h4 id="참고-자료"><a href="#참고-자료" class="headerlink" title="참고 자료"></a>참고 자료</h4><ul>
<li><a href="https://datascienceschool.net/view-notebook/3f44cfdda2874080a9aa6b034c71d5ec/" target="_blank" rel="noopener">데이터 사이언스 스쿨 - 벡터와 행렬의 연산</a></li>
<li><a href="https://datascienceschool.net/view-notebook/927d7f7972dd434ead9d294169ae7f34/" target="_blank" rel="noopener">데이터 사이언스 스쿨 - 선형 연립방정식과 역행렬</a></li>
</ul>

  </div>
</article>

    <div class="blog-post-comments">
        <div id="disqus_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/categories/Diary">Diary</a></li>
         
          <li><a href="/categories/DataScience/">Data Science</a></li>
         
          <li><a href="/categories/Life/">Life</a></li>
         
          <li><a href="/categories/">Categories</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-선형-회귀-모형-linear-regression"><span class="toc-number">1.</span> <span class="toc-text">1. 선형 회귀 모형(linear regression)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-선형-회귀-모형이란"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 선형 회귀 모형이란</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-잔차"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 잔차</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-잔차-제곱합-RSS-residual-sum-of-squares"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 잔차 제곱합(RSS: residual sum of squares)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-연립방정식과-역행렬"><span class="toc-number">2.</span> <span class="toc-text">2. 연립방정식과 역행렬</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-선형-연립-방정식-system-of-linear-equations"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 선형 연립 방정식(system of linear equations)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-역행렬"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 역행렬</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-역행렬과-선형-연립-방정식의-해"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 역행렬과 선형 연립 방정식의 해</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-최소-자승-문제-Least-Square-Problem"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 최소 자승 문제 (Least Square Problem)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#미지수의-수와-방정식의-수를-고려한-연립-방정식의-종류"><span class="toc-number">2.4.1.</span> <span class="toc-text">미지수의 수와 방정식의 수를 고려한 연립 방정식의 종류</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#최소-자승-문제"><span class="toc-number">2.4.2.</span> <span class="toc-text">최소 자승 문제</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#의사-역행렬-pseudo-inverse"><span class="toc-number">2.4.3.</span> <span class="toc-text">의사 역행렬(pseudo inverse)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#참고-자료"><span class="toc-number">2.4.4.</span> <span class="toc-text">참고 자료</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&text=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&title=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&is_video=false&description=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)&body=Check out this article: https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&title=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&title=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&title=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&title=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/&name=&lt;선형대수&gt; 선형대수와 선형회귀모형(linear regression)&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 우와팬더
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/categories/Diary">Diary</a></li>
         
          <li><a href="/categories/DataScience/">Data Science</a></li>
         
          <li><a href="/categories/Life/">Life</a></li>
         
          <li><a href="/categories/">Categories</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<!-- clipboard -->

  <script src="/lib/clipboard/clipboard.min.js"></script>
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>

<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-126954393-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

<!-- Disqus Comments -->

    <script type="text/javascript">
        var disqus_shortname = 'hyeshinoh';

        (function(){
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }());
    </script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
